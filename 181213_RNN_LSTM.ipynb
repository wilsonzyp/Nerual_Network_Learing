{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN循环神经网络的直观理解：基于TensorFlow的简单RNN例子\n",
    "https://blog.csdn.net/weiwei9363/article/details/78902455\n",
    "## RNN的结构\n",
    "如果从网上搜索关于RNN的结构图，大概可以下面的结构图 \n",
    "![img1](./img/20171226150421731.png)\n",
    "第一次看到这样的图，我是懵逼的，这货怎么有两种形态? 先说结论：\n",
    "\n",
    "左侧是RNN的原始结构， 右侧是RNN在时间上展开的结果\n",
    "\n",
    "RNN的结构，本质上和全连接网络相同\n",
    "\n",
    "为什么可以根据时间维度展开，这主要是因为RNN的的输入是具有时间序列的。这一点是和全连接网络最大的不同，输入决定了RNN的结构 \n",
    "\n",
    "假设RNN的输入是一句话，这句话中有多个单词，那么RNN需要forward多次，如下图 \n",
    "![img2](./img/20171226150439177.gif)\n",
    "\n",
    "橙色部分是上一个时刻的隐层的值，可以直观的理解为“记忆”\n",
    "\n",
    "当前时刻的输出与当前时刻的输入还有记忆有关。\n",
    "\n",
    "RNN对一个样本需要做多次forward，这一点与全连接网络不一样，全连接网络对一个样本只做一次forward。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 就将RNN看成是全连接网络吧\n",
    "将RNN看成是全连接网络就能很好的理解它。\n",
    "\n",
    "将上面gif图中的隐层中的每一个神经元看成是LSTM单元，就得到了基于LSTM的RNN\n",
    "\n",
    "RNN的输入、输出都和全连接网络一模一样\n",
    "\n",
    "RNN只是一个需要做好多次forward的全连接网络\n",
    "\n",
    "一个RNN的简单例子\n",
    "基于TensorFlow，搭建一个RNN，教会神经网络进行二进制加法。参考Anyone Can learn To Code LSTM-RNN in Python(Part 1: RNN)https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 一个字典，隐射一个数字到其二进制的表示\n",
    "# 例如 int2binary[3] = [0,0,0,0,0,0,1,1]\n",
    "int2binary = {}\n",
    "\n",
    "# 最多8位二进制\n",
    "binary_dim = 8\n",
    "\n",
    "# 在8位情况下，最大数为2^8 = 256\n",
    "largest_number = pow(2,binary_dim)\n",
    "\n",
    "# 将[0,256)所有数表示成二进制\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "\n",
    "# 建立字典\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "def binary_generation(numbers, reverse = False):\n",
    "    '''\n",
    "    返回numbers中所有数的二进制表达，\n",
    "    例如 numbers = [3, 2, 1]\n",
    "    返回 ：[[0,0,0,0,0,0,1,1],\n",
    "            [0,0,0,0,0,0,1,0],\n",
    "            [0,0,0,0,0,0,0,1]'\n",
    "\n",
    "    如果 reverse = True, 二进制表达式前后颠倒，\n",
    "    这么做是为训练方便，因为训练的输入顺序是从低位开始的\n",
    "\n",
    "    numbers : 一组数字\n",
    "    reverse : 是否将其二进制表示进行前后翻转\n",
    "    '''\n",
    "    binary_x = np.array([ int2binary[num] for num in numbers], dtype=np.uint8)\n",
    "\n",
    "    if reverse:\n",
    "        binary_x = np.fliplr(binary_x)\n",
    "\n",
    "    return binary_x\n",
    "\n",
    "def batch_generation(batch_size, largest_number):\n",
    "    '''\n",
    "    生成batch_size大小的数据，用于训练或者验证\n",
    "\n",
    "    batch_x 大小为[batch_size, biniary_dim, 2]\n",
    "    batch_y 大小为[batch_size, biniray_dim]\n",
    "    '''\n",
    "\n",
    "    # 随机生成batch_size个数\n",
    "    n1 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    n2 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    # 计算加法结果\n",
    "    add = n1 + n2\n",
    "\n",
    "    # int to binary\n",
    "    binary_n1 = binary_generation(n1, True)\n",
    "    binary_n2 = binary_generation(n2, True)\n",
    "    batch_y = binary_generation(add, True)\n",
    "\n",
    "    # 堆叠，因为网络的输入是2个二进制\n",
    "    batch_x = np.dstack((binary_n1, binary_n2))\n",
    "\n",
    "    return batch_x, batch_y, n1, n2, add\n",
    "\n",
    "def binary2int(binary_array):\n",
    "    '''\n",
    "    将一个二进制数组转为整数\n",
    "    '''\n",
    "    out = 0\n",
    "    for index, x in enumerate(reversed(binary_array)):\n",
    "        out += x*pow(2, index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# LSTM的个数，就是隐层中神经元的数量\n",
    "lstm_size = 20\n",
    "# 隐层的层数\n",
    "lstm_layers =2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入，[None, binary_dim, 2], \n",
    "# None表示batch_size, binary_dim表示输入序列的长度，2表示每个时刻有两个输入\n",
    "x = tf.placeholder(tf.float32, [None, binary_dim, 2], name='input_x')\n",
    "\n",
    "# 输出\n",
    "y_ = tf.placeholder(tf.float32, [None, binary_dim], name='input_y')\n",
    "# dropout 参数\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建LSTM层（看成隐层）\n",
    "# 有lstm_size个单元\n",
    "lstm = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "# dropout\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "# 一层不够，就多来几层\n",
    "def lstm_cell():\n",
    "  return tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([ lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "# 初始状态，可以理解为初始记忆\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# 进行forward，得到隐层的输出\n",
    "# outputs 大小为[batch_size, lstm_size*binary_dim]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state)\n",
    "\n",
    "# 建立输出层\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size, 1], stddev=0.01))\n",
    "bias = tf.zeros([1])\n",
    "\n",
    "# [batch_size, lstm_size*binary_dim] ==> [batch_size*binary_dim, lstm_size]\n",
    "outputs = tf.reshape(outputs, [-1, lstm_size])\n",
    "# 得到输出, logits大小为[batch_size*binary_dim, 1]\n",
    "logits = tf.sigmoid(tf.matmul(outputs, weights))\n",
    "# [batch_size*binary_dim, 1] ==> [batch_size, binary_dim]\n",
    "predictions = tf.reshape(logits, [-1, binary_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.mean_squared_error(y_, predictions)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:1000, Loss:0.08420679718255997\n",
      "Iter:2000, Loss:0.0028582673985511065\n",
      "[0 0 1 0 0 0 1 1]:35\n",
      "[0 0 1 1 0 0 1 1]:51\n",
      "[0 1 0 1 0 1 1 0]:86\n",
      "\n",
      "[0 0 0 0 0 0 0 1]:1\n",
      "[0 1 1 0 0 0 1 1]:99\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "[0 0 0 0 1 0 0 1]:9\n",
      "[0 0 1 1 1 1 1 1]:63\n",
      "\n",
      "[0 1 0 0 1 1 1 0]:78\n",
      "[0 0 1 0 0 1 0 0]:36\n",
      "[0 1 1 1 0 0 1 0]:114\n",
      "\n",
      "[0 0 0 0 1 0 1 1]:11\n",
      "[0 0 0 1 1 0 0 0]:24\n",
      "[0 0 1 0 0 0 1 1]:35\n",
      "\n",
      "[0 1 1 1 0 0 0 0]:112\n",
      "[0 0 0 0 1 0 1 0]:10\n",
      "[0 1 1 1 1 0 1 0]:122\n",
      "\n",
      "[0 0 0 0 0 1 1 0]:6\n",
      "[0 1 0 0 1 0 0 1]:73\n",
      "[0 1 0 0 1 1 1 1]:79\n",
      "\n",
      "[0 1 1 0 1 1 0 1]:109\n",
      "[0 1 0 1 1 0 0 0]:88\n",
      "[1 1 0 0 0 1 0 1]:197\n",
      "\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "[0 0 1 0 1 1 1 1]:47\n",
      "[1 0 0 1 1 0 0 0]:152\n",
      "\n",
      "[0 0 1 1 0 0 1 1]:51\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 0 1 0 1 0 1 0]:170\n",
      "\n",
      "[0 0 1 0 1 1 0 1]:45\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 0 1 0 0 1 0 0]:164\n",
      "\n",
      "[0 1 1 0 0 0 1 0]:98\n",
      "[0 1 1 0 0 1 1 1]:103\n",
      "[1 1 0 0 1 0 0 1]:201\n",
      "\n",
      "[0 1 0 0 0 1 0 1]:69\n",
      "[0 0 0 1 1 0 0 1]:25\n",
      "[0 1 0 1 1 1 1 0]:94\n",
      "\n",
      "[0 1 1 1 0 0 1 0]:114\n",
      "[0 0 1 1 1 0 0 1]:57\n",
      "[1 0 1 0 1 0 1 1]:171\n",
      "\n",
      "[0 1 1 1 1 1 0 1]:125\n",
      "[0 0 0 0 0 1 0 1]:5\n",
      "[1 0 0 0 0 0 1 0]:130\n",
      "\n",
      "[0 1 0 0 0 0 0 1]:65\n",
      "[0 0 0 0 0 0 1 0]:2\n",
      "[0 1 0 0 0 0 1 1]:67\n",
      "\n",
      "[0 1 0 1 1 0 0 0]:88\n",
      "[0 1 1 0 0 0 1 0]:98\n",
      "[1 0 1 1 1 0 1 0]:186\n",
      "\n",
      "[0 0 1 1 1 0 1 1]:59\n",
      "[0 1 0 1 0 1 1 1]:87\n",
      "[1 0 0 1 0 0 1 0]:146\n",
      "\n",
      "[0 0 0 0 0 1 0 0]:4\n",
      "[0 1 1 0 0 0 0 0]:96\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "\n",
      "[0 1 1 0 0 1 0 1]:101\n",
      "[0 1 0 0 0 1 1 0]:70\n",
      "[1 0 1 0 1 0 1 1]:171\n",
      "\n",
      "[0 1 1 1 1 1 0 0]:124\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "[1 1 1 0 1 0 1 1]:235\n",
      "\n",
      "[0 1 1 1 1 0 0 1]:121\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 1 1 1 0 0 0 0]:240\n",
      "\n",
      "[0 1 1 0 0 0 0 0]:96\n",
      "[0 0 1 0 0 1 0 1]:37\n",
      "[1 0 0 0 0 1 0 1]:133\n",
      "\n",
      "[0 0 0 1 0 1 1 1]:23\n",
      "[0 0 0 1 0 0 0 0]:16\n",
      "[0 0 1 0 0 1 1 1]:39\n",
      "\n",
      "[0 0 1 1 0 0 0 1]:49\n",
      "[0 0 1 0 1 0 0 0]:40\n",
      "[0 1 0 1 1 0 0 1]:89\n",
      "\n",
      "[0 1 1 0 1 1 1 0]:110\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[1 0 0 0 1 1 1 1]:143\n",
      "\n",
      "[0 0 1 1 0 0 1 0]:50\n",
      "[0 1 0 1 1 1 0 1]:93\n",
      "[1 0 0 0 1 1 1 1]:143\n",
      "\n",
      "[0 0 1 0 0 1 0 1]:37\n",
      "[0 1 0 1 1 1 0 1]:93\n",
      "[1 0 0 0 0 0 1 0]:130\n",
      "\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "[0 0 1 1 0 1 0 1]:53\n",
      "[1 0 1 0 1 0 0 0]:168\n",
      "\n",
      "[0 0 1 0 0 1 1 1]:39\n",
      "[0 1 0 0 1 1 1 0]:78\n",
      "[0 1 1 1 0 1 0 1]:117\n",
      "\n",
      "[0 1 0 0 0 0 1 1]:67\n",
      "[0 1 0 1 0 1 0 1]:85\n",
      "[1 0 0 1 1 0 0 0]:152\n",
      "\n",
      "[0 0 1 0 1 1 1 1]:47\n",
      "[0 1 0 1 1 1 0 0]:92\n",
      "[1 0 0 0 1 0 1 1]:139\n",
      "\n",
      "[0 0 0 1 1 0 0 0]:24\n",
      "[0 1 1 0 0 0 0 0]:96\n",
      "[0 1 1 1 1 0 0 0]:120\n",
      "\n",
      "[0 0 0 1 1 0 1 1]:27\n",
      "[0 1 0 0 1 1 1 1]:79\n",
      "[0 1 1 0 1 0 1 0]:106\n",
      "\n",
      "[0 1 0 1 1 0 0 0]:88\n",
      "[0 1 0 1 1 1 1 1]:95\n",
      "[1 0 1 1 0 1 1 1]:183\n",
      "\n",
      "[0 1 0 1 1 1 0 1]:93\n",
      "[0 0 0 0 1 1 0 0]:12\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[0 1 0 1 1 1 0 0]:92\n",
      "[1 0 0 1 1 0 0 0]:152\n",
      "\n",
      "[0 1 0 1 0 1 0 1]:85\n",
      "[0 0 1 0 0 0 1 1]:35\n",
      "[0 1 1 1 1 0 0 0]:120\n",
      "\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "[0 0 0 0 1 1 0 0]:12\n",
      "[0 1 1 1 1 1 1 1]:127\n",
      "\n",
      "[0 1 1 1 1 0 1 1]:123\n",
      "[0 1 0 0 0 1 1 1]:71\n",
      "[1 1 0 0 0 0 1 0]:194\n",
      "\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "[0 0 0 0 1 0 1 0]:10\n",
      "[0 1 1 0 1 1 1 0]:110\n",
      "\n",
      "[0 0 1 0 1 1 1 0]:46\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "\n",
      "[0 1 0 0 0 1 1 1]:71\n",
      "[0 0 1 1 0 0 1 0]:50\n",
      "[0 1 1 1 1 0 0 1]:121\n",
      "\n",
      "[0 0 1 0 0 0 0 0]:32\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "[1 0 0 1 0 0 1 1]:147\n",
      "\n",
      "[0 1 1 1 1 1 1 1]:127\n",
      "[0 1 0 1 1 1 1 0]:94\n",
      "[1 1 0 1 1 1 0 1]:221\n",
      "\n",
      "[0 1 0 1 1 1 0 0]:92\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[1 0 0 1 1 0 0 0]:152\n",
      "\n",
      "[0 0 0 0 0 1 0 1]:5\n",
      "[0 0 0 1 0 1 1 1]:23\n",
      "[0 0 0 1 1 1 0 0]:28\n",
      "\n",
      "[0 1 1 1 1 0 0 0]:120\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "[1 0 1 0 1 1 1 0]:174\n",
      "\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "[0 0 1 0 1 0 0 1]:41\n",
      "[1 0 0 1 0 0 1 0]:146\n",
      "\n",
      "[0 0 1 0 1 1 0 1]:45\n",
      "[0 1 1 1 0 0 0 0]:112\n",
      "[1 0 0 1 1 1 0 1]:157\n",
      "\n",
      "[0 1 0 0 1 1 1 1]:79\n",
      "[0 1 0 1 1 1 0 1]:93\n",
      "[1 0 1 0 1 1 0 0]:172\n",
      "\n",
      "[0 0 1 1 0 0 0 1]:49\n",
      "[0 0 1 1 1 0 0 0]:56\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "\n",
      "[0 1 0 1 0 0 0 0]:80\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[0 1 1 1 0 0 0 1]:113\n",
      "\n",
      "[0 0 0 1 0 1 1 1]:23\n",
      "[0 1 1 1 0 1 0 1]:117\n",
      "[1 0 0 0 1 1 0 0]:140\n",
      "\n",
      "[0 1 0 0 1 1 1 1]:79\n",
      "[0 0 1 0 0 0 0 0]:32\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "\n",
      "[0 0 1 0 0 0 0 0]:32\n",
      "[0 0 1 1 0 0 1 0]:50\n",
      "[0 1 0 1 0 0 1 0]:82\n",
      "\n",
      "[0 0 1 1 1 0 1 1]:59\n",
      "[0 0 1 0 0 0 1 1]:35\n",
      "[0 1 0 1 1 1 1 0]:94\n",
      "\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[0 1 1 0 1 0 1 1]:107\n",
      "[1 1 1 0 0 0 1 0]:226\n",
      "\n",
      "[0 0 1 0 1 0 0 1]:41\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "[1 0 0 1 1 0 0 0]:152\n",
      "\n",
      "[0 0 0 1 1 0 1 1]:27\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "\n",
      "[0 0 1 0 0 0 1 0]:34\n",
      "[0 1 1 1 0 1 0 0]:116\n",
      "[1 0 0 1 0 1 1 0]:150\n",
      "\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "[0 0 0 0 1 1 1 1]:15\n",
      "[1 0 0 0 0 0 1 0]:130\n",
      "\n",
      "[0 0 1 1 0 1 0 0]:52\n",
      "[0 0 0 0 0 0 1 0]:2\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "\n",
      "[0 0 1 0 1 0 0 0]:40\n",
      "[0 0 0 1 1 0 0 0]:24\n",
      "[0 1 0 0 0 0 0 0]:64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 2000\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    iteration = 1\n",
    "    for i in range(steps):\n",
    "        # 获取训练数据\n",
    "        input_x, input_y,_,_,_ = batch_generation(batch_size, largest_number)\n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={x:input_x, y_:input_y, keep_prob:0.5})\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print('Iter:{}, Loss:{}'.format(iteration, loss))    \n",
    "        iteration += 1\n",
    "\n",
    "    # 训练结束，进行测试\n",
    "    val_x, val_y, n1, n2, add = batch_generation(batch_size, largest_number)\n",
    "    result = sess.run(predictions, feed_dict={x:val_x, y_:val_y, keep_prob:1.0})\n",
    "\n",
    "    # 左右翻转二进制数组。因为输出的结果是低位在前，而正常的表达是高位在前，因此进行翻转\n",
    "    result = np.fliplr(np.round(result))\n",
    "    result = result.astype(np.int32)\n",
    "\n",
    "    for  b_x, b_p, a, b, add in zip(np.fliplr(val_x), result, n1, n2, add):\n",
    "        print('{}:{}'.format(b_x[:,0], a))\n",
    "        print('{}:{}'.format(b_x[:,1], b))\n",
    "        print('{}:{}\\n'.format(b_p, binary2int(b_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "本文向大家介绍了RNN的结构，以及从全连接网络的角度出去，去理解RNN，得到结论有：\n",
    "\n",
    "RNN的结构与全连接网络基本一致\n",
    "RNN具有时间展开的特点，这是由其输入决定的\n",
    "全连接网络对一个样本做一次forward，RNN对一个样本做多次forward\n",
    "同时，基于TensorFlow给出一个简单的RNN网络，从实验结果看，该网络经过训练，以及能够正确进行二进制加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [1 1 1 ..., 1 0 1]\n",
      " [1 1 1 ..., 1 1 0]\n",
      " [1 1 1 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "print(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "   18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "   36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "   54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "   72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "   90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      "  108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      "  126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      "  144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      "  162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      "  180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      "  198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      "  216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      "  234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      "  252 253 254 255]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([range(largest_number)],dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
